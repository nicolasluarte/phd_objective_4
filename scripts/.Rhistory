ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0) +
geom_point()
temperature_mdl_emm <- emmeans::emmeans(
temperature_mdl,
specs = ~ entropy_level | vir,
type = "response"
) %>% emmeans::test()
temperature_mdl_emm
## temperature mdl----
temperature_mdl <- glmmTMB::glmmTMB(
data = diff_rl_mdl_data,
temperature ~ entropy_level * vir + (1  + entropy_level + vir || ID),
family = glmmTMB::beta_family(link = "logit")
)
summary(temperature_mdl)
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0) +
geom_point()
temperature_mdl_emm <- emmeans::emmeans(
temperature_mdl,
specs = ~ entropy_level | vir,
type = "response"
) %>% emmeans::test()
temperature_mdl_emm
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0.5) +
geom_point()
# likelihood function ----
likelihood_function <- function(
theta, # theta[1] learning rate, theta[2] temperature
actions,
rewards
){
# parameters
alpha <- theta[1]
tau <- theta[2]
# this vector will store the log-probabilities
# this is a vector of all 0's
logp_actions_t <- numeric(length(actions))
# initialize the Q table, with total indifference or random
random_init = FALSE
if (random_init == TRUE){
L = round(runif(1, min=0, max=1), 1)
R = round(runif(1, min=0, max=1), 1)
Q <- list(c(L, R))
}
else{
Q <- c(0.5, 0.66)
}
# do simulations
for (i in 1:length(actions)){
# apply softmax but in log form
soft_max_out <- soft_max(Q, tau)
# a=1 left, a=2 right
r <- rewards[i]
a <- actions[i]
# store log probability of each action
logp_actions_t[i] <- soft_max_out[a]
# update Q values
Q[a] <- Q[a] + (alpha * (r - Q[a]))
}
mu <- 1e-9
penalization_tau <- -mu * (log(tau - 0.001) + log(5 - tau))
penalization_alpha <- -mu * (log(alpha - 0.001) + log(1 - alpha))
penalization_sum <- penalization_alpha + penalization_tau
total_penalization <- if_else(penalization_sum==Inf||is.na(penalization_sum), 1e9, penalization_sum)
return(-sum(logp_actions_t[-1]) + total_penalization)
}
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm_100.cm_100.na_na_na_na" ~ "baseline",
treatment == "cm_100.cm_100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm_100.cm_100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm_50.cm_100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_50.cm_100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_100.cm_50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_100.cm_50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_25.cm_50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_25.cm_50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm_50.cm_25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_50.cm_25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
) %>%
filter(actividad != -1, treatment != "baseline") %>%
group_by(ID, treatment, sensor) %>%
mutate(
new_event = if_else(evento!=lag(evento) & evento > 0, 1, 0),
new_reward = if_else(exito!=lag(exito), 1, 0)
) %>%
ungroup() %>%
filter(new_event==1) %>%
group_by(ID, treatment, evento) %>%
mutate(
actions = sensor+1,
rewards = max(new_reward, na.rm = TRUE)
) %>%
ungroup() %>%
select(ID, actions, rewards, treatment, n_sesion) %>%
group_by(ID, treatment, n_sesion) %>%
drop_na() %>%
group_split()
write_csv(x = ed_choice %>% bind_rows(), "../datasets/individual_choice_data_DREADDS.csv")
# model fit ----
# do at least 1000 runs
plan(multisession, workers = 12)
model_fits <- 1:100 %>%
future_map_dfr(., function(iteration){
it <- map_dfr(ed_choice, function(X){
dat_sim <- X
lower_bounds <- c(alpha = 0.001, tau = 0.001) # Use small positive values
upper_bounds <- c(alpha = 1, tau = 5)      # Reasonable upper bounds
start_par <- c(
runif(1, min = lower_bounds[1], max = upper_bounds[1]),
runif(1, min = lower_bounds[2], max = upper_bounds[2])
)
param_optim <- optim(
par = start_par,
fn = likelihood_function,
method = "L-BFGS-B",
lower = lower_bounds,
upper = upper_bounds,
actions = dat_sim$actions,
rewards = dat_sim$rewards
)
optim_res <- param_optim$par
return(tibble(
opt_alpha = optim_res[1],
opt_tau = optim_res[2],
likelihood = param_optim$value,
iteration = iteration,
ID = dat_sim$ID[1],
treatment = dat_sim$treatment[1],
session = dat_sim$n_sesion[1]
))
})
return(it)
}, .options = furrr_options(seed = 6911))
write_rds(model_fits, "../datasets/RL_model_fits_DREADDS.rds")
# read the data of fitted values
optimal_mdl_fit <- read_rds("../datasets/RL_model_fits_DREADDS.rds") %>%
mutate(
drug = replace_na(str_extract(treatment, "veh|cno"), "no_drug"),
entropy_level = replace_na(str_extract(treatment, "[a-z]+_[a-z]+"), "low_entropy")
) %>%
group_by(ID, drug, entropy_level, session) %>%
slice(which.min(likelihood)) %>%
mutate(vir = if_else(ID %in% c(809, 810, 811), "trt", "ctrl"))
optimal_mdl_fit
optimal_mdl_fit %>%
filter(entropy_level == "high_entropy") %>%
group_by(ID, entropy_level, drug, session) %>%
summarise(
m = mean(opt_tau)
)
# individual choice data to compute reward-side entropy
binary_entropy <- function(p) {
if (p == 0 || p == 1) {
return(0)
}
return(- (p * log2(p) + (1 - p) * log2(1 - p)))
}
ind_dat <- read_csv("../datasets/individual_choice_data_DREADDS.csv") %>%
ungroup() %>%
group_by(ID, treatment) %>%
summarise(
H = binary_entropy(mean(rewards))
)
ind_dat
# this is to set everything to the same scale
# this makes sense because parameters are derived from a bounded
# optimization process
rl_mdl_data <- optimal_mdl_fit %>%
ungroup() %>%
mutate(
learning_rate = (opt_alpha * (n() - 1) + 0.5) / n(),
temperature = ((opt_tau/5)* (n() - 1) + 0.5) / n(),
num_ent = as.numeric(factor(as.factor(entropy_level), levels = c("low_entropy", "mid_entropy", "high_entropy")))
) %>%
left_join(., ind_dat, by = c("ID", "treatment"))
mm <- glmmTMB::glmmTMB(
data = rl_mdl_data %>% filter(vir == "trt"),
temperature ~ entropy_level * drug + (1 | ID),
family = glmmTMB::beta_family(link = "logit")
)
summary(mm)
emmeans::emmeans(
mm,
pairwise ~ drug | entropy_level,
type = "link"
)
param_data <- rl_mdl_data %>%
mutate(
drug = relevel(as.factor(drug), ref = "veh"),
vir = relevel(as.factor(vir), ref = "ctrl"),
entropy_level = relevel(as.factor(entropy_level), ref = "low_entropy")
)
mm <- lmerTest::lmer(
data = param_data %>% filter(drug!="no_drug"),
log(temperature) ~ entropy_level * drug * vir + (1|ID)
)
summary(mm)
diff_rl_mdl_data <- rl_mdl_data %>%
ungroup() %>%
filter(drug != "no_drug") %>%
group_by(ID, entropy_level, vir) %>%
mutate(
temperature = (temperature - temperature[drug=="cno"]),
learning_rate = (learning_rate - learning_rate[drug=="cno"]),
temperature = 1 / (1 + exp(-temperature))
) %>%
filter(drug=="veh")
diff_rl_mdl_data
mm <- lmerTest::lmer(
data = diff_rl_mdl_data,
temperature ~ entropy_level * vir + (1|ID)
)
summary(mm)
## temperature mdl----
temperature_mdl <- glmmTMB::glmmTMB(
data = diff_rl_mdl_data,
temperature ~ entropy_level * vir + (1  + entropy_level + vir || ID),
family = glmmTMB::beta_family(link = "logit")
)
summary(temperature_mdl)
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0.5) +
geom_point()
temperature_mdl_emm <- emmeans::emmeans(
temperature_mdl,
specs = ~ entropy_level | vir,
type = "response"
) %>% emmeans::test()
temperature_mdl_emm
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0.5) +
geom_point() +
scale_y_continuous(transform = "log")
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0.5) +
geom_label(aes(label=ID))
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0.5) +
geom_label(aes(label=ID)) +
facet_wrap(~ID)
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0.5) +
geom_label(aes(label=ID)) +
facet_wrap(~ID, scale = "free_y")
diff_rl_mdl_data
View(diff_rl_mdl_data)
# likelihood function ----
likelihood_function <- function(
theta, # theta[1] learning rate, theta[2] temperature
actions,
rewards
){
# parameters
alpha <- theta[1]
tau <- theta[2]
# this vector will store the log-probabilities
# this is a vector of all 0's
logp_actions_t <- numeric(length(actions))
# initialize the Q table, with total indifference or random
random_init = FALSE
if (random_init == TRUE){
L = round(runif(1, min=0, max=1), 1)
R = round(runif(1, min=0, max=1), 1)
Q <- list(c(L, R))
}
else{
Q <- c(0.5, 0.67)
}
# do simulations
for (i in 1:length(actions)){
# apply softmax but in log form
soft_max_out <- soft_max(Q, tau)
# a=1 left, a=2 right
r <- rewards[i]
a <- actions[i]
# store log probability of each action
logp_actions_t[i] <- soft_max_out[a]
# update Q values
Q[a] <- Q[a] + (alpha * (r - Q[a]))
}
mu <- 1e-9
penalization_tau <- -mu * (log(tau - 0.001) + log(5 - tau))
penalization_alpha <- -mu * (log(alpha - 0.001) + log(1 - alpha))
penalization_sum <- penalization_alpha + penalization_tau
total_penalization <- if_else(penalization_sum==Inf||is.na(penalization_sum), 1e9, penalization_sum)
return(-sum(logp_actions_t[-1]) + total_penalization)
}
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm_100.cm_100.na_na_na_na" ~ "baseline",
treatment == "cm_100.cm_100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm_100.cm_100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm_50.cm_100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_50.cm_100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_100.cm_50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_100.cm_50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_25.cm_50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_25.cm_50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm_50.cm_25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_50.cm_25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
) %>%
filter(actividad != -1, treatment != "baseline") %>%
group_by(ID, treatment, sensor) %>%
mutate(
new_event = if_else(evento!=lag(evento) & evento > 0, 1, 0),
new_reward = if_else(exito!=lag(exito), 1, 0)
) %>%
ungroup() %>%
filter(new_event==1) %>%
group_by(ID, treatment, evento) %>%
mutate(
actions = sensor+1,
rewards = max(new_reward, na.rm = TRUE)
) %>%
ungroup() %>%
select(ID, actions, rewards, treatment, n_sesion) %>%
group_by(ID, treatment, n_sesion) %>%
drop_na() %>%
group_split()
write_csv(x = ed_choice %>% bind_rows(), "../datasets/individual_choice_data_DREADDS.csv")
# model fit ----
# do at least 1000 runs
plan(multisession, workers = 12)
model_fits <- 1:100 %>%
future_map_dfr(., function(iteration){
it <- map_dfr(ed_choice, function(X){
dat_sim <- X
lower_bounds <- c(alpha = 0.001, tau = 0.001) # Use small positive values
upper_bounds <- c(alpha = 1, tau = 5)      # Reasonable upper bounds
start_par <- c(
runif(1, min = lower_bounds[1], max = upper_bounds[1]),
runif(1, min = lower_bounds[2], max = upper_bounds[2])
)
param_optim <- optim(
par = start_par,
fn = likelihood_function,
method = "L-BFGS-B",
lower = lower_bounds,
upper = upper_bounds,
actions = dat_sim$actions,
rewards = dat_sim$rewards
)
optim_res <- param_optim$par
return(tibble(
opt_alpha = optim_res[1],
opt_tau = optim_res[2],
likelihood = param_optim$value,
iteration = iteration,
ID = dat_sim$ID[1],
treatment = dat_sim$treatment[1],
session = dat_sim$n_sesion[1]
))
})
return(it)
}, .options = furrr_options(seed = 6911))
write_rds(model_fits, "../datasets/RL_model_fits_DREADDS.rds")
model_fits %>%
group_by(treatment) %>%
summarise(
est = mean(opt_tau)
)
model_fits %>%
filter(likelihood != 0) %>%
group_by(ID, treatment) %>%
slice(which.min(likelihood)) %>%
view()
# read the data of fitted values
optimal_mdl_fit <- read_rds("../datasets/RL_model_fits_DREADDS.rds") %>%
mutate(
drug = replace_na(str_extract(treatment, "veh|cno"), "no_drug"),
entropy_level = replace_na(str_extract(treatment, "[a-z]+_[a-z]+"), "low_entropy")
) %>%
group_by(ID, drug, entropy_level, session) %>%
slice(which.min(likelihood)) %>%
mutate(vir = if_else(ID %in% c(809, 810, 811), "trt", "ctrl"))
optimal_mdl_fit
optimal_mdl_fit %>%
filter(entropy_level == "high_entropy") %>%
group_by(ID, entropy_level, drug, session) %>%
summarise(
m = mean(opt_tau)
)
# individual choice data to compute reward-side entropy
binary_entropy <- function(p) {
if (p == 0 || p == 1) {
return(0)
}
return(- (p * log2(p) + (1 - p) * log2(1 - p)))
}
ind_dat <- read_csv("../datasets/individual_choice_data_DREADDS.csv") %>%
ungroup() %>%
group_by(ID, treatment) %>%
summarise(
H = binary_entropy(mean(rewards))
)
ind_dat
# this is to set everything to the same scale
# this makes sense because parameters are derived from a bounded
# optimization process
rl_mdl_data <- optimal_mdl_fit %>%
ungroup() %>%
mutate(
learning_rate = (opt_alpha * (n() - 1) + 0.5) / n(),
temperature = ((opt_tau/5)* (n() - 1) + 0.5) / n(),
num_ent = as.numeric(factor(as.factor(entropy_level), levels = c("low_entropy", "mid_entropy", "high_entropy")))
) %>%
left_join(., ind_dat, by = c("ID", "treatment"))
mm <- glmmTMB::glmmTMB(
data = rl_mdl_data %>% filter(vir == "trt"),
temperature ~ entropy_level * drug + (1 | ID),
family = glmmTMB::beta_family(link = "logit")
)
summary(mm)
emmeans::emmeans(
mm,
pairwise ~ drug | entropy_level,
type = "link"
)
param_data <- rl_mdl_data %>%
mutate(
drug = relevel(as.factor(drug), ref = "veh"),
vir = relevel(as.factor(vir), ref = "ctrl"),
entropy_level = relevel(as.factor(entropy_level), ref = "low_entropy")
)
mm <- lmerTest::lmer(
data = param_data %>% filter(drug!="no_drug"),
log(temperature) ~ entropy_level * drug * vir + (1|ID)
)
summary(mm)
diff_rl_mdl_data <- rl_mdl_data %>%
ungroup() %>%
filter(drug != "no_drug") %>%
group_by(ID, entropy_level, vir) %>%
mutate(
temperature = (temperature - temperature[drug=="cno"]),
learning_rate = (learning_rate - learning_rate[drug=="cno"]),
temperature = 1 / (1 + exp(-temperature))
) %>%
filter(drug=="veh")
diff_rl_mdl_data
mm <- lmerTest::lmer(
data = diff_rl_mdl_data,
temperature ~ entropy_level * vir + (1|ID)
)
summary(mm)
## temperature mdl----
temperature_mdl <- glmmTMB::glmmTMB(
data = diff_rl_mdl_data,
temperature ~ entropy_level * vir + (1  + entropy_level + vir || ID),
family = glmmTMB::beta_family(link = "logit")
)
summary(temperature_mdl)
diff_rl_mdl_data %>%
ggplot(aes(
entropy_level, temperature, fill = vir
)) +
geom_hline(yintercept = 0.5) +
geom_label(aes(label=ID)) +
facet_wrap(~ID, scale = "free_y")
temperature_mdl_emm <- emmeans::emmeans(
temperature_mdl,
specs = ~ entropy_level | vir,
type = "response"
) %>% emmeans::test()
temperature_mdl_emm
## temperature mdl----
temperature_mdl <- glmmTMB::glmmTMB(
data = diff_rl_mdl_data,
temperature ~ entropy_level * vir + (1  + entropy_level + vir || ID),
family = glmmTMB::beta_family(link = "logit")
)
