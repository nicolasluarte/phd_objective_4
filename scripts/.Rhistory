xlab(
latex2exp::TeX(r"($\alpha$)")
)
tau_alpha_p
# p::tau by alpha ----
tau_alpha_p <- tau_alpha_emm %>%
ggplot(aes(
learning_rate, estimate
)) +
geom_ribbon(aes(ymin=conf.low, ymax=conf.high),
alpha = 0.05) +
geom_line() +
scale_x_continuous(breaks = seq(0, 1, 0.25)) +
theme_uncertainty +
scale_y_continuous(breaks = seq(-1.5, 1.5, 0.5),
limits = c(-1.5, 1.5),
expand = c(0,0)) +
ylab(
latex2exp::TeX(r"($\hat{\tau}$)")
) +
xlab(
latex2exp::TeX(r"($\alpha$)")
)
tau_alpha_p
tau_alpha_emm
# p::tau by alpha ----
tau_alpha_p <- tau_alpha_emm %>%
ggplot(aes(
learning_rate, estimate
)) +
geom_ribbon(aes(ymin=conf.low, ymax=conf.high),
alpha = 0.05) +
geom_line() +
scale_x_continuous(breaks = seq(0, 1, 0.25)) +
theme_uncertainty +
scale_y_continuous(breaks = seq(-3, 3, 0.5),
limits = c(-3, 3),
expand = c(0,0)) +
ylab(
latex2exp::TeX(r"($\hat{\tau}$)")
) +
xlab(
latex2exp::TeX(r"($\alpha$)")
)
tau_alpha_p
# p::tau by alpha ----
tau_alpha_p <- tau_alpha_emm %>%
ggplot(aes(
learning_rate, estimate
)) +
geom_ribbon(aes(ymin=conf.low, ymax=conf.high),
alpha = 0.05) +
geom_line() +
scale_x_continuous(breaks = seq(0, 1, 0.25)) +
theme_uncertainty +
scale_y_continuous(breaks = seq(-2.5, 2.5, 0.5),
limits = c(-2.5, 2.5),
expand = c(0,0)) +
ylab(
latex2exp::TeX(r"($\hat{\tau}$)")
) +
xlab(
latex2exp::TeX(r"($\alpha$)")
)
tau_alpha_p
taup + alphap + tau_alpha_p + tau_h
taup + alphap + tau_alpha_p + tau_H
taup <- temperature_mdl_emm %>%
mutate(entropy_level = factor(as.factor(entropy_level),
levels = c("low_entropy", "mid_entropy", "high_entropy"))) %>%
ggplot(aes(
entropy_level, estimate
)) +
geom_pointrange(aes(ymin=conf.low, ymax=conf.high),
size = 1.25) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_point(aes(entropy_level, temperature),
data = diff_rl_mdl_data,
fill = "orange"
) +
annotate("text", label="*", x=3, y=1.2, size=12) +
scale_x_discrete(labels = c(
latex2exp::TeX(r"($H_{low}$)"),
latex2exp::TeX(r"($H_{mid}$)"),
latex2exp::TeX(r"($H_{high}$)")
)) +
theme_uncertainty +
scale_y_continuous(breaks = seq(-1.5, 1.5, 0.5),
limits = c(-1.5, 1.5),
expand = c(0,0)) +
ylab(
latex2exp::TeX(r"($\hat{\tau}_{tcs - veh}$)")
) +
xlab("")
taup
taup + alphap + tau_alpha_p + tau_H
|
(taup | alphap) + tau_alpha_p + tau_H
taup + alphap + tau_alpha_p + tau_H +
plot_layout(widths = c(1, 1, 2, 2))
taup + alphap + tau_alpha_p + tau_H +
plot_layout(widths = c(1, 1, 2, 2)) +
plot_annotation(tag_levels = c("A"))
#libs ----
pacman::p_load(
tidyverse,
ggplot2,
lme4,
lmerTest,
furrr,
ggpubr,
patchwork,
lme4,
lmerTest,
zeallot,
nloptr,
robustlmm,
cmdstanr,
posterior
)
setwd(this.path::here())
# Plot stuff ----
theme_uncertainty <- ggpubr::theme_pubr() +
update_geom_defaults("point", list(size = 5, alpha = 0.5, shape = 21)) +
update_geom_defaults("boxplot", list(width=0.5)) +
theme(
text = element_text(size = 24),
axis.text=element_text(size=14),
plot.margin = unit(c(0.5,0.5,0.5,0.5), "cm"),
legend.position = "none"
)
# soft-max action selection function ----
soft_max <- function(Q_vector, tau){
# we compute the log of probabilities to get a stable response
# we do not need to log Q because log(exp(x)) = x
# so note that that I did not apply exp to Q_exp
# because later on if would log(exp(Q_scaled))
# 1/tau because Im multiplying and I want to use temperature not inverse temp
Q_scaled <- (as.vector(Q_vector) * (1/tau))
# use the log rule for division
# and do the log-sum-exp trick to avoid under and overflows
soft_max_out <- Q_scaled - matrixStats::logSumExp(Q_scaled)
return(soft_max_out)
}
# likelihood function ----
likelihood_function <- function(
theta, # theta[1] learning rate, theta[2] temperature
actions,
rewards
){
# parameters
alpha <- theta[1]
tau <- theta[2]
# this vector will store the log-probabilities
# this is a vector of all 0's
logp_actions_t <- numeric(length(actions))
# initialize the Q table, with total indifference or random
random_init = FALSE
if (random_init == TRUE){
L = round(runif(1, min=0, max=1), 1)
R = round(runif(1, min=0, max=1), 1)
Q <- list(c(L, R))
}
else{
Q <- c(0.5, 0.5)
}
# do simulations
for (i in 1:length(actions)){
# apply softmax but in log form
soft_max_out <- soft_max(Q, tau)
# a=1 left, a=2 right
r <- rewards[i]
a <- actions[i]
# store log probability of each action
logp_actions_t[i] <- soft_max_out[a]
# update Q values
Q[a] <- Q[a] + (alpha * (r - Q[a]))
}
return(-sum(logp_actions_t[-1]))
}
# empirical data ----
ed <- read_csv("../datasets/lickometer_complete.csv")
ed
# empirical data ----
devtools::source_url("https://github.com/lab-cpl/lickometer-library/blob/main/src/lickometer_functions_compilate.R?raw=TRUE")
load_experiment
d <- load_experiment(
metadataFileName = "../datasets/metadata_DREADDS/metadata.csv",
data_directory_path = "../datasets/lickometer_DREADDS"
)
d
tmp <- load_experiment(
metadataFileName = "../datasets/metadata_DREADDS/metadata.csv",
data_directory_path = "../datasets/lickometer_DREADDS"
)
write_csv(x = tmp, file = "../datasets/lickometer_complete_DREADDS.csv")
ed <- read_csv("../datasets/lickometer_complete_DREADDS.csv")
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm100.cm100.na_na_na_na" ~ "baseline",
treatment == "cm100.cm100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm100.cm100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm50.cm100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm50.cm100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm100.cm50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm100.cm50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm25.cm50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm25.cm50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm50.cm25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm50.cm25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
)
View(ed_choice)
ed_choice$treatment %>% unique
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm100_cm100_na_na_na_na" ~ "baseline",
treatment == "cm100_cm100_veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm100_cm100_cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm50_cm100_veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm50_cm100_cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm100_cm50_veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm100_cm50_cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm25_cm50_veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm25_cm50_cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm50_cm25_veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm50_cm25_cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
)
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm100.cm100.na_na_na_na" ~ "baseline",
treatment == "cm100.cm100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm100.cm100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm50.cm100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm50.cm100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm100.cm50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm100.cm50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm25.cm50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm25.cm50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm50.cm25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm50.cm25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
)
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm_100.cm_100.na_na_na_na" ~ "baseline",
treatment == "cm_100.cm_100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm_100.cm_100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm_50.cm_100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_50.cm_100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_100.cm_50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_100.cm_50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_25.cm_50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_25.cm_50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm_50.cm_25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_50.cm_25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
)
tmp <- load_experiment(
metadataFileName = "../datasets/metadata_DREADDS/metadata.csv",
data_directory_path = "../datasets/lickometer_DREADDS"
)
tmp <- load_experiment(
metadataFileName = "../datasets/metadata_DREADDS/metadata.csv",
data_directory_path = "../datasets/lickometer_DREADDS"
)
write_csv(x = tmp, file = "../datasets/lickometer_complete_DREADDS.csv")
ed <- read_csv("../datasets/lickometer_complete_DREADDS.csv")
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm_100.cm_100.na_na_na_na" ~ "baseline",
treatment == "cm_100.cm_100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm_100.cm_100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm_50.cm_100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_50.cm_100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_100.cm_50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_100.cm_50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_25.cm_50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_25.cm_50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm_50.cm_25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_50.cm_25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
) %>%
filter(actividad != -1, treatment != "baseline") %>%
group_by(ID, treatment, sensor) %>%
mutate(
new_event = if_else(evento!=lag(evento) & evento > 0, 1, 0),
new_reward = if_else(exito!=lag(exito), 1, 0)
) %>%
ungroup() %>%
filter(new_event==1) %>%
group_by(ID, treatment, evento) %>%
mutate(
actions = sensor+1,
rewards = max(new_reward, na.rm = TRUE)
) %>%
ungroup() %>%
select(ID, actions, rewards, treatment) %>%
group_by(ID, treatment) %>%
drop_na() %>%
group_split()
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm_100.cm_100.na_na_na_na" ~ "baseline",
treatment == "cm_100.cm_100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm_100.cm_100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm_50.cm_100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_50.cm_100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_100.cm_50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_100.cm_50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_25.cm_50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_25.cm_50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm_50.cm_25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_50.cm_25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
)
ed_choice <- ed %>%
group_by(ID, fecha) %>%
arrange(tiempo, .by_group = TRUE) %>%
ungroup() %>%
mutate(
treatment = interaction(estimulo_spout_1, estimulo_spout_2, droga),
treatment = case_when(
treatment == "cm_100.cm_100.na_na_na_na" ~ "baseline",
treatment == "cm_100.cm_100.veh_na_na_na" ~ "low_entropy_veh",
treatment == "cm_100.cm_100.cno_na_na_na" ~ "low_entropy_cno",
treatment == "cm_50.cm_100.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_50.cm_100.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_100.cm_50.veh_na_na_na" ~ "mid_entropy_veh",
treatment == "cm_100.cm_50.cno_na_na_na" ~ "mid_entropy_cno",
treatment == "cm_25.cm_50.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_25.cm_50.cno_na_na_na" ~ "high_entropy_cno",
treatment == "cm_50.cm_25.veh_na_na_na" ~ "high_entropy_veh",
treatment == "cm_50.cm_25.cno_na_na_na" ~ "high_entropy_cno",
TRUE ~ "ERROR"
)
) %>%
filter(actividad != -1, treatment != "baseline") %>%
group_by(ID, treatment, sensor) %>%
mutate(
new_event = if_else(evento!=lag(evento) & evento > 0, 1, 0),
new_reward = if_else(exito!=lag(exito), 1, 0)
) %>%
ungroup() %>%
filter(new_event==1) %>%
group_by(ID, treatment, evento) %>%
mutate(
actions = sensor+1,
rewards = max(new_reward, na.rm = TRUE)
) %>%
ungroup() %>%
select(ID, actions, rewards, treatment) %>%
group_by(ID, treatment) %>%
drop_na() %>%
group_split()
write_csv(x = ed_choice %>% bind_rows(), "../datasets/individual_choice_data_DREADDS.csv")
ed_choice
# model fit ----
# do at least 1000 runs
plan(multisession, workers = 12)
model_fits <- 1:1000 %>%
future_map_dfr(., function(iteration){
it <- map_dfr(ed_choice, function(X){
dat_sim <- X
lower_bounds <- c(alpha = 0.001, tau = 0.001) # Use small positive values
upper_bounds <- c(alpha = 1, tau = 5)      # Reasonable upper bounds
start_par <- c(
runif(1, min = lower_bounds[1], max = upper_bounds[1]),
runif(1, min = lower_bounds[2], max = upper_bounds[2])
)
param_optim <- optim(
par = start_par,
fn = likelihood_function,
method = "L-BFGS-B",
lower = lower_bounds,
upper = upper_bounds,
actions = dat_sim$actions,
rewards = dat_sim$rewards
)
optim_res <- param_optim$par
return(tibble(
opt_alpha = optim_res[1],
opt_tau = optim_res[2],
likelihood = param_optim$value,
iteration = iteration,
ID = dat_sim$ID[1],
treatment = dat_sim$treatment[1]
))
})
return(it)
}, .options = furrr_options(seed = 6911))
write_rds(model_fits, "../datasets/RL_model_fits_DREADDS.rds")
# lib load ----
pacman::p_load(
tidyverse,
ggplot2,
patchwork
)
setwd(this.path::here())
# Plot stuff ----
theme_uncertainty <- ggpubr::theme_pubr() +
update_geom_defaults("point", list(size = 5, alpha = 0.5, shape = 21)) +
theme(
text = element_text(size = 24),
axis.text=element_text(size=14),
plot.margin = unit(c(0.5,0.5,0.5,0.5), "cm"),
legend.position = "none"
)
boxplot_sig_bracket <- function(group1, group2){
ggsignif::geom_signif(
comparisons = list(c(group1, group2)),
map_signif_level = TRUE,
textsize = 0,
tip_length = 0
)
}
ld <- read_csv("../datasets/lickometer_complete.csv")
ld <- read_csv("../datasets/lickometer_complete_DREADDS.csv")
# read the data of fitted values slice the best fit
# DO NOT AVERAGE OUT !!
# filter out likelihoods of exactly 0 these are optimization errors
optimal_mdl_fit <- read_rds("../datasets/RL_model_fits_DREADDS.rds") %>%
#    filter(likelihood != 0) %>%
group_by(ID, treatment) %>%
slice(which.min(likelihood)) %>%
mutate(
drug = str_extract(treatment, "veh|cno"),
entropy_level = str_extract(treatment, "[a-z]+_[a-z]+")
)
optimal_mdl_fit
# individual choice data to compute reward-side entropy
binary_entropy <- function(p) {
if (p == 0 || p == 1) {
return(0)
}
return(- (p * log2(p) + (1 - p) * log2(1 - p)))
}
ind_dat <- read_csv("../datasets/individual_choice_data_DREADDS.csv") %>%
ungroup() %>%
group_by(ID, treatment) %>%
summarise(
H = binary_entropy(mean(rewards))
)
ind_dat
# this is to set everything to the same scale
# this makes sense because parameters are derived from a bounded
# optimization process
rl_mdl_data <- optimal_mdl_fit %>%
ungroup() %>%
mutate(
learning_rate = (opt_alpha * (n() - 1) + 0.5) / n(),
temperature = ((opt_tau/5)* (n() - 1) + 0.5) / n()
) %>%
left_join(., ind_dat, by = c("ID", "treatment"))
# used beta to inform the model of hard boundaries impose in the
# optimization procedure
# could not compute alpha for 574
diff_rl_mdl_data <- rl_mdl_data %>%
ungroup() %>%
group_by(ID) %>%
mutate(
temperature = (temperature - temperature[drug=="tcs"]),
learning_rate = (learning_rate - learning_rate[drug=="tcs"])
) %>%
filter(drug=="veh")
diff_rl_mdl_data
rl_mdl_data
ind_dat
optima_mdl_fit
optimal_mdl_fit
ed_choice[[5]]
ed_choice[[6]]
ed_choice[[6]] %>% view
ed_choice[[3]] %>% view
ed_choice[[4]] %>% view
